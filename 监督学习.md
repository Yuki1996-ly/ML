1.selfsupervised-learning

我们将有label标注的资料叫做supervised learning，没有lable标注的叫unsupervised learning。

对于从自己数据里面拿了一部分出来标注的叫做self-supervised learning

<img src="C:\Users\admin\Documents\GitHub\ML\Photoes\image-20230714113030156.png" alt="image-20230714113030156" style="zoom:25%;" />

1.2这里输入四个字，中文的字对应的是一个token，将某一个字盖住，盖住可以使用一个特殊的符号取代也可以是随机替换一个字，然后进行通过函数算出分类，取出最接近的值。

<img src="C:\Users\admin\Documents\GitHub\ML\Photoes\image-20230714114942823.png" alt="image-20230714114942823" style="zoom:25%;" />



Bert虽然被训练来做填空题的预测，或者是下一个句子的预测。但它实际是被用来做一些下游的任务，

<img src="C:\Users\admin\Documents\GitHub\ML\Photoes\image-20230714120838589.png" alt="image-20230714120838589" style="zoom:25%;" />

在做下游任务的时候，还是需要一些资料的。就像一个干细胞，在有了一些资料后，可以变成各种各样的细胞。也叫fine-tune。当然产生Bert这个过程叫做Pre-train,fine-tune的下一个阶段是reinforcement learning(通过真实的使用者来进行训练)

<img src="C:\Users\admin\Documents\GitHub\ML\Photoes\image-20230714121010412.png" alt="image-20230714121010412" style="zoom:25%;" />

<img src="C:\Users\admin\Documents\GitHub\ML\Photoes\image-20230714121027302.png" alt="image-20230714121027302" style="zoom:25%;" />

第一个例子

输入一个句子，输出一个类别。

通过Bert微调一个模型，将训练好的Bert参数给到Bert(做完形填空的Bert)，这里在通过函数去做微调，这里的参数是随机的。当然这里还是需要有一些参数的，因为Bert是不知道要做的任务是什么样的。

<img src="C:\Users\admin\Documents\GitHub\ML\Photoes\image-20230714121756876.png" alt="image-20230714121756876" style="zoom:25%;" />

case2:可以用在句子分类

<img src="C:\Users\admin\Documents\GitHub\ML\Photoes\image-20230714122504805.png" alt="image-20230714122504805" style="zoom:25%;" />

case3 推理:可以用在立场分析，比如评论

<img src="C:\Users\admin\Documents\GitHub\ML\Photoes\image-20230714122652343.png" alt="image-20230714122652343" style="zoom:25%;" />

case4：这里其实就是通过向量去寻找s和e

<img src="C:\Users\admin\Documents\GitHub\ML\Photoes\image-20230714123214570.png" alt="image-20230714123214570" style="zoom:25%;" />



将问题和文章给到Bert，去做向量的计算寻找最大值。

这里橙色的部分表示起始的向量，蓝色表示结束的向量。橙色向量和文章做计算取出最大值，和蓝色向量和文章的向量做计算寻求最大值，这中间的内容就是答案



<img src="C:\Users\admin\Documents\GitHub\ML\Photoes\image-20230714123301644.png" alt="image-20230714123301644" style="zoom:25%;" />

GPT系列

GPT就是通过不断预测下一个token产生完整的文章，因为GPT之前产生了独角兽的假新闻，还活灵活现了，所以有些人以独角兽形象表示独角兽

<img src="C:\Users\admin\Documents\GitHub\ML\Photoes\image-20230714124231553.png" alt="image-20230714124231553" style="zoom: 25%;" />



GPT具体的使用例子，给出例子，然后它有泛化的能力

<img src="C:\Users\admin\Documents\GitHub\ML\Photoes\image-20230714124630091.png" alt="image-20230714124630091" style="zoom:25%;" />